#!/usr/bin/env python
try:
    import simplejson as json
except ImportError:
    import json
from datetime import datetime
from traceback import print_exc
import math, sys, os
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from subprocess import Popen, PIPE
# Import histogram specs and generated by makefile using specgen.py
# This is imported the zipfile that this module was loaded from
#HistogramAggregator = __loader__.load_module("auxiliary").HistogramAggregator
from auxiliary import HistogramAggregator

# Counts number of times we've printed a log message
logMsgCount = {}

# Auxiliary method to write log messages
def log(msg, *args):
    global logMsgCount
    # We only print a log message the first 10 times we see it
    n = logMsgCount.get(msg, 10)
    if n > 0:
        logMsgCount[msg] = n - 1
        print >> sys.stderr, msg % args


############################## Ugly hacks for simple measures

# Auxiliary method for computing bucket offsets from parameters, it is stolen
# from histogram_tools.py, though slightly modified...
def exponential_buckets(dmin, dmax, n_buckets):
    log_max = math.log(dmax);
    ret_array = [0] * n_buckets
    current = dmin
    ret_array[1] = current
    for bucket_index in range(2, n_buckets):
        log_current = math.log(current)
        log_ratio = (log_max - log_current) / (n_buckets - bucket_index)
        log_next = log_current + log_ratio
        next_value = int(math.floor(math.exp(log_next) + 0.5))
        if next_value > current:
            current = next_value
        else:
            current = current + 1
        ret_array[bucket_index] = current
    return ret_array

# Create buckets from buckets2index from ranges... snippet pretty much stolen
# from specgen.py
def buckets2index_from_ranges(ranges):
    buckets = map(str, ranges)
    bucket2index = {}
    for i in range(0, len(buckets)):
        bucket2index[buckets[i]] = i
    return bucket2index

# Bucket offsets for simple measures
simple_measures_buckets =   (
                                buckets2index_from_ranges(
                                        exponential_buckets(1, 30000, 50)
                                    ),
                                    exponential_buckets(1, 30000, 50)
                            )

############################## End of ugly hacks for simple measures

class DashboardProcessor:
    def __init__(self, output_folder):
        self.output_folder = output_folder
        self.cache = {}

    def process(self, uid, dimensions, payload):
        # Unpack dimensions
        reason, appName, channel, version, buildId, submissionDate = dimensions

        # Get OS, osVersion and architecture information
        info = payload['info']
        OS = info['OS']
        osVersion = str(info['version'])
        arch = info['arch']
        revision = info['revision']

        # Get the major version
        majorVersion = version.split('.')[0]

        if OS == "Linux":
            osVersion = osVersion[:3]

        # Get the build date, ignore the rest of the buildId
        buildDate = buildId[:8]


        filterPath = (buildDate, reason, appName, OS, osVersion, arch)
        self.create_aggregates(filterPath, channel, majorVersion, "by-build-date",
                               payload, buildId, revision)

        # Aggregate histograms by submission date, if time since build is less
        # than 60 days
        bdate = datetime.strptime(buildDate, "%Y%m%d")
        sdate = datetime.strptime(submissionDate, "%Y%m%d")
        if (sdate - bdate).days < 60:
            filterPath = (buildDate, reason, appName, OS, osVersion, arch)

            self.create_aggregates(filterPath, channel, majorVersion,
                                   "by-submission-date", payload, buildId,
                                   revision)

    def create_aggregates(self, filterPath, channel, majorVersion, byDateType, payload, buildId, revision):
        # Aggregate histograms
        for name, values in payload.get('histograms', {}).iteritems():
            filePath = (channel, majorVersion, name, byDateType)

            # Find cache set for filePath
            cacheSet = self.cache.setdefault(filePath, {})
            # Find aggregator for filter path
            aggregator = cacheSet.get(filterPath, None)
            if aggregator == None:
                aggregator = HistogramAggregator()
                cacheSet[filterPath] = aggregator

            # Aggregate values
            aggregator.merge(values + [1], buildId, revision)

        # Aggregate simple measurements
        for name, value in payload.get('simpleMeasurements', {}).iteritems():
            # Handle cases where the value is a dictionary of simple measures
            if type(value) == dict:
                for subName, subValue in value.iteritems():
                    self.aggregate_simple_measure(channel, majorVersion, filterPath,
                                                  name + "_" + str(subName),
                                                  byDateType, subValue)
            else:
                self.aggregate_simple_measure(channel, majorVersion, filterPath,
                                              name, byDateType, value)

    def aggregate_simple_measure(self, channel, majorVersion, filterPath, name,
                                 byDateType, value):
        # Sanity check value
        if type(value) not in (int, long, float):
            log("%s is not a value type for simpleMeasurements \"%s\"",
                type(value), name)
            return

        bucket = simple_measures_buckets[1]
        values = [0] * (len(bucket) + 6)
        for i in reversed(range(0, len(bucket))):
            if value >= bucket[i]:
                values[i] = 1
                break

        log_val = math.log(math.fabs(value) + 1)
        values[-6] = value                # sum
        values[-5] = log_val              # log_sum
        values[-4] = log_val * log_val    # log_sum_squares
        values[-3] = 0                    # sum_squares_lo
        values[-2] = 0                    # sum_squares_hi
        values[-1] = 1                    # count


        filePath = (channel, majorVersion, "SIMPLE_MEASURES_" + name.upper(), byDateType)

        # Find cache set for filePath
        cacheSet = self.cache.setdefault(filePath, {})
        # Find aggregator for filter path
        aggregator = cacheSet.get(filterPath, None)
        if aggregator == None:
            aggregator = HistogramAggregator()
            cacheSet[filterPath] = aggregator

        # Aggregate values
        aggregator.merge(values, "0", "simple-measures-hack")

    def flush(self):
        with open(os.path.join(self.output_folder, "result.txt"), "w") as out:
            for filePath, cacheSet in self.cache.iteritems():
                output = {}
                for filterPath, aggregator in cacheSet.iteritems():
                    output["/".join(filterPath)] = aggregator.dump()
                out.write("/".join(filePath) + "\t" + json.dumps(output) + "\n")

############################## Driver

def main():
    """ The processor, reading input file-names from stdin """
    p = ArgumentParser(
        description = 'Analyze files as they are given by stdin',
        formatter_class = ArgumentDefaultsHelpFormatter
    )
    p.add_argument(
        "-o", "--output-folder",
        help = "Location to put final output"
    )
    cfg = p.parse_args()

    processor = DashboardProcessor(cfg.output_folder)

    # Handle input as it is fetched
    for line in sys.stdin:
        # Find s3 prefix (key) and physical filepath
        prefix, filepath = line.split("\t")
        filepath = filepath.strip()

        # Find dimensions
        dims = prefix.split('/')
        dims += dims.pop().split('.')[:2]

        # Open a compressor
        raw_handle = open(filepath, "rb")
        decompressor = Popen(
            ['xz', '-d', '-c'],
            bufsize = 65536,
            stdin = raw_handle,
            stdout = PIPE,
            stderr = sys.stderr
        )

        # Process each line
        line_nb = 0
        errors = 0
        for line in decompressor.stdout:
            line_nb += 1
            try:
                uid, payload = line.split("\t", 1)
                payload = json.loads(payload)
                processor.process(uid, dims, payload)
            except:
                print >> sys.stderr, ("Bad input line: %i of %s" %
                                      (line_nb, prefix))
                print_exc(file = sys.stderr)
                errors += 1

        # Close decompressor
        decompressor.stdout.close()
        raw_handle.close()

        # Remove file after processing
        os.remove(filepath)

    # Write aggregates to disk
    processor.flush()

if __name__ == "__main__":
    sys.exit(main())